{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.2 beautifulsoup 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.3 기본 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\n  <meta charset=\"utf-8\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\\n  <meta name=\"description\" content=\"\">\\n  <meta name=\"author\" content=\"\">\\n\\n  <title>Clean Blog - 곰돌이 Theme</title>\\n\\n  <!-- Bootstrap core CSS -->\\n  <link href=\"vendor/bootstrap/css/bootstrap.min.css\" rel=\"stylesheet\">\\n\\n  <!-- Custom fonts for this template -->\\n  <link href=\"vendor/fontawesome-free/css/all.min.css\" rel=\"stylesheet\" type=\"text/css\">\\n  <link href=\\'https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic\\' rel=\\'stylesheet\\' type=\\'text/css\\'>\\n  <link href=\\'https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800\\' rel=\\'stylesheet\\' type=\\'text/css\\'>\\n\\n  <!-- Custom styles for this template -->\\n  <link href=\"css/clean-blog.min.css\" rel=\"stylesheet\">\\n\\n</head>\\n\\n<body>\\n\\n  <!-- Navigation -->\\n  <nav class=\"navbar navbar-expand-lg navbar-light fixed-top\" id=\"mainNav\">\\n    <div class=\"container\">\\n      <a class=\"navbar-brand\" href=\"index.html\">Home</a>     \\n    </div>\\n  </nav>\\n\\n  <!-- Page Header -->\\n  <header class=\"masthead\" style=\"background-image: url(\\'img/home-bg.jpg\\')\">\\n    <div class=\"overlay\"></div>\\n    <div class=\"container\">\\n      <div class=\"row\">\\n        <div class=\"col-lg-8 col-md-10 mx-auto\">\\n          <div class=\"site-heading\">\\n            <h1>곰돌이 블로그</h1>\\n            <span class=\"subheading\">곰돌이 블로그에 오신 것을 환영합니다.</span>\\n          </div>\\n        </div>\\n      </div>\\n    </div>\\n  </header>\\n\\n  <!-- Main Content -->\\n  <div class=\"container\">\\n    <div class=\"row\">\\n      <div class=\"col-lg-8 col-md-10 mx-auto\">\\n        <div class=\"post-preview\">\\n          <a href=\"index.html\">\\n            <h2 class=\"post-title\">\\n              단풍구경가자~~\\n            </h2>\\n            <h3 class=\"post-subtitle\">\\n              동키랑 단풍구경 다녀왔어요\\n            </h3>\\n          </a>\\n          <p class=\"post-meta\">September 24, 2021</p>\\n        </div>\\n        <hr>\\n        <div class=\"post-preview\">\\n          <a href=\"index.html\">\\n            <h2 class=\"post-title\">\\n              다이어트 시작!\\n            </h2>\\n            <h3 class=\"post-subtitle\">\\n              꿀은 이제 그만, 날씬한 곰돌이로 다시 태어나기\\n            </h3>          \\n          </a>\\n          <p class=\"post-meta\">June 18, 2021</p>\\n        </div>\\n        <hr>\\n        <div class=\"post-preview\">\\n          <a href=\"index.html\">\\n            <h2 class=\"post-title\">\\n              꽃놀이간다!\\n            </h2>\\n            <h3 class=\"post-subtitle\">\\n              화창한 봄날, 티거랑 함께 산책간 이야기\\n            </h3>\\n          </a>\\n          <p class=\"post-meta\">March 24, 2021</p>\\n        </div>\\n        <hr>\\n        <div class=\"post-preview\">\\n          <a href=\"index.html\">\\n            <h2 class=\"post-title\">\\n              내 꿀은 누가 다 먹었나?\\n            </h2>\\n            <h3 class=\"post-subtitle\">\\n              없어진 꿀의 행방을 찾아라\\n            </h3>\\n          </a>\\n          <p class=\"post-meta\">July 8, 2020</p>\\n        </div>\\n        <hr>\\n        <!-- Pager -->\\n        <div class=\"clearfix\">\\n          <a class=\"btn btn-primary float-right\" href=\"#\">Older Posts &rarr;</a>\\n        </div>\\n      </div>\\n    </div>\\n  </div>\\n\\n  <hr>\\n\\n  <!-- Footer -->\\n  <footer>\\n    <div class=\"container\">\\n      <div class=\"row\">\\n        <div class=\"col-lg-8 col-md-10 mx-auto\">\\n          <p class=\"copyright text-muted\">Copyright &copy; My Website 2021</p>\\n        </div>\\n      </div>\\n    </div>\\n  </footer>\\n</body>\\n\\n</html>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = r\"C:\\Users\\USER\\Desktop\\workspace01\\skn_1\\crawling_\\크롤링예제\\index.html\"\n",
    "html = \"\"\n",
    "with open (filename, 'r', encoding='UTF-8') as file:  \n",
    "    for line in file:               \n",
    "        html += line\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASCII_SPACES', 'DEFAULT_BUILDER_FEATURES', 'DEFAULT_INTERESTING_STRING_TYPES', 'EMPTY_ELEMENT_EVENT', 'END_ELEMENT_EVENT', 'NO_PARSER_SPECIFIED_WARNING', 'ROOT_TAG_NAME', 'START_ELEMENT_EVENT', 'STRING_ELEMENT_EVENT', '__bool__', '__call__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_strings', '_clone', '_decode_markup', '_event_stream', '_feed', '_find_all', '_find_one', '_format_tag', '_indent_string', '_is_xml', '_lastRecursiveChild', '_last_descendant', '_linkage_fixer', '_markup_is_url', '_markup_resembles_filename', '_most_recent_element', '_namespaces', '_popToTag', '_should_pretty_print', 'append', 'attrs', 'builder', 'can_be_empty_element', 'cdata_list_attributes', 'childGenerator', 'children', 'clear', 'contains_replacement_characters', 'contents', 'css', 'currentTag', 'current_data', 'declared_html_encoding', 'decode', 'decode_contents', 'decompose', 'decomposed', 'default', 'descendants', 'element_classes', 'encode', 'encode_contents', 'endData', 'extend', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'formatter_for_name', 'get', 'getText', 'get_attribute_list', 'get_text', 'handle_data', 'handle_endtag', 'handle_starttag', 'has_attr', 'has_key', 'hidden', 'index', 'insert', 'insert_after', 'insert_before', 'interesting_string_types', 'isSelfClosing', 'is_empty_element', 'is_xml', 'known_xml', 'markup', 'name', 'namespace', 'new_string', 'new_tag', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_element', 'next_elements', 'next_sibling', 'next_siblings', 'object_was_parsed', 'open_tag_counter', 'original_encoding', 'parent', 'parentGenerator', 'parents', 'parse_only', 'parserClass', 'parser_class', 'popTag', 'prefix', 'preserve_whitespace_tag_stack', 'preserve_whitespace_tags', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_element', 'previous_elements', 'previous_sibling', 'previous_siblings', 'pushTag', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'reset', 'select', 'select_one', 'self_and_descendants', 'setup', 'smooth', 'string', 'string_container', 'string_container_stack', 'strings', 'stripped_strings', 'tagStack', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# print(dir(soup))\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"post-title\">\n",
       "              단풍구경가자~~\n",
       "            </h2>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"post-title\">\n",
       "               단풍구경가자~~\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               다이어트 시작!\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               꽃놀이간다!\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               내 꿀은 누가 다 먹었나?\n",
       "             </h2>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트로 반환\n",
    "soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n              단풍구경가자~~\\n            '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'단풍구경가자~~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단풍구경가자~~\n",
      "다이어트 시작!\n",
      "꽃놀이간다!\n",
      "내 꿀은 누가 다 먹었나?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['단풍구경가자~~', '다이어트 시작!', '꽃놀이간다!', '내 꿀은 누가 다 먹었나?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list = soup.find_all('h2')\n",
    "\n",
    "title1 = []\n",
    "for title in title_list:\n",
    "    print(title.text.strip())\n",
    "    title1.append(title.text.strip())\n",
    "title1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September 24, 2021\n",
      "June 18, 2021\n",
      "March 24, 2021\n",
      "July 8, 2020\n",
      "Copyright © My Website 2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['September 24, 2021', 'June 18, 2021', 'March 24, 2021', 'July 8, 2020']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list = soup.find_all('p')\n",
    "정리된date = []\n",
    "for date in date_list:\n",
    "    print(date.text.strip())\n",
    "    정리된date.append(date.text.strip())\n",
    "정리된date.pop()\n",
    "정리된date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"post-meta\">September 24, 2021</p>,\n",
       " <p class=\"post-meta\">June 18, 2021</p>,\n",
       " <p class=\"post-meta\">March 24, 2021</p>,\n",
       " <p class=\"post-meta\">July 8, 2020</p>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class = .\n",
    "# id = #\n",
    "\n",
    "date_list = soup.find_all('p', {'class': 'post-meta'})\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September 24, 2021\n",
      "June 18, 2021\n",
      "March 24, 2021\n",
      "July 8, 2020\n"
     ]
    }
   ],
   "source": [
    "for date in date_list:\n",
    "    print(date.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version\n",
      "--------------------------------- ------------\n",
      "aiobotocore                       2.7.0\n",
      "aiohttp                           3.9.3\n",
      "aioitertools                      0.7.1\n",
      "aiosignal                         1.2.0\n",
      "alabaster                         0.7.12\n",
      "altair                            5.0.1\n",
      "anaconda-anon-usage               0.4.3\n",
      "anaconda-catalogs                 0.2.0\n",
      "anaconda-client                   1.12.3\n",
      "anaconda-cloud-auth               0.1.4\n",
      "anaconda-navigator                2.5.2\n",
      "anaconda-project                  0.11.1\n",
      "anyio                             4.2.0\n",
      "appdirs                           1.4.4\n",
      "archspec                          0.2.1\n",
      "argon2-cffi                       21.3.0\n",
      "argon2-cffi-bindings              21.2.0\n",
      "arrow                             1.2.3\n",
      "astroid                           2.14.2\n",
      "astropy                           5.3.4\n",
      "asttokens                         2.0.5\n",
      "async-lru                         2.0.4\n",
      "atomicwrites                      1.4.0\n",
      "attrs                             23.1.0\n",
      "Automat                           20.2.0\n",
      "autopep8                          1.6.0\n",
      "Babel                             2.11.0\n",
      "backports.functools-lru-cache     1.6.4\n",
      "backports.tempfile                1.0\n",
      "backports.weakref                 1.0.post1\n",
      "bcrypt                            3.2.0\n",
      "beautifulsoup4                    4.12.2\n",
      "binaryornot                       0.4.4\n",
      "black                             23.11.0\n",
      "bleach                            4.1.0\n",
      "blinker                           1.6.2\n",
      "bokeh                             3.3.4\n",
      "boltons                           23.0.0\n",
      "botocore                          1.31.64\n",
      "Bottleneck                        1.3.7\n",
      "Brotli                            1.0.9\n",
      "cachetools                        4.2.2\n",
      "certifi                           2024.2.2\n",
      "cffi                              1.16.0\n",
      "chardet                           4.0.0\n",
      "charset-normalizer                2.0.4\n",
      "click                             8.1.7\n",
      "cloudpickle                       2.2.1\n",
      "clyent                            1.2.2\n",
      "colorama                          0.4.6\n",
      "colorcet                          3.0.1\n",
      "comm                              0.1.2\n",
      "conda                             24.1.2\n",
      "conda-build                       24.1.2\n",
      "conda-content-trust               0.2.0\n",
      "conda_index                       0.4.0\n",
      "conda-libmamba-solver             24.1.0\n",
      "conda-pack                        0.6.0\n",
      "conda-package-handling            2.2.0\n",
      "conda_package_streaming           0.9.0\n",
      "conda-repo-cli                    1.0.75\n",
      "conda-token                       0.4.0\n",
      "conda-verify                      3.4.2\n",
      "constantly                        23.10.4\n",
      "contourpy                         1.2.0\n",
      "cookiecutter                      2.5.0\n",
      "cryptography                      42.0.2\n",
      "cssselect                         1.2.0\n",
      "cycler                            0.11.0\n",
      "cytoolz                           0.12.2\n",
      "dask                              2023.11.0\n",
      "datashader                        0.16.0\n",
      "debugpy                           1.6.7\n",
      "decorator                         5.1.1\n",
      "defusedxml                        0.7.1\n",
      "diff-match-patch                  20200713\n",
      "dill                              0.3.7\n",
      "distributed                       2023.11.0\n",
      "distro                            1.8.0\n",
      "docstring-to-markdown             0.11\n",
      "docutils                          0.18.1\n",
      "entrypoints                       0.4\n",
      "et-xmlfile                        1.1.0\n",
      "executing                         0.8.3\n",
      "fastjsonschema                    2.16.2\n",
      "filelock                          3.13.1\n",
      "flake8                            6.0.0\n",
      "Flask                             2.2.5\n",
      "fonttools                         4.25.0\n",
      "frozenlist                        1.4.0\n",
      "fsspec                            2023.10.0\n",
      "future                            0.18.3\n",
      "gensim                            4.3.0\n",
      "gitdb                             4.0.7\n",
      "GitPython                         3.1.37\n",
      "gmpy2                             2.1.2\n",
      "greenlet                          3.0.1\n",
      "h5py                              3.9.0\n",
      "HeapDict                          1.0.1\n",
      "holoviews                         1.18.3\n",
      "hvplot                            0.9.2\n",
      "hyperlink                         21.0.0\n",
      "idna                              3.4\n",
      "imagecodecs                       2023.1.23\n",
      "imageio                           2.33.1\n",
      "imagesize                         1.4.1\n",
      "imbalanced-learn                  0.11.0\n",
      "importlib-metadata                7.0.1\n",
      "incremental                       22.10.0\n",
      "inflection                        0.5.1\n",
      "iniconfig                         1.1.1\n",
      "intake                            0.6.8\n",
      "intervaltree                      3.1.0\n",
      "ipykernel                         6.28.0\n",
      "ipython                           8.20.0\n",
      "ipython-genutils                  0.2.0\n",
      "ipywidgets                        7.6.5\n",
      "isort                             5.9.3\n",
      "itemadapter                       0.3.0\n",
      "itemloaders                       1.1.0\n",
      "itsdangerous                      2.0.1\n",
      "jaraco.classes                    3.2.1\n",
      "jedi                              0.18.1\n",
      "jellyfish                         1.0.1\n",
      "Jinja2                            3.1.3\n",
      "jmespath                          1.0.1\n",
      "joblib                            1.2.0\n",
      "json5                             0.9.6\n",
      "jsonpatch                         1.32\n",
      "jsonpointer                       2.1\n",
      "jsonschema                        4.19.2\n",
      "jsonschema-specifications         2023.7.1\n",
      "jupyter                           1.0.0\n",
      "jupyter_client                    8.6.0\n",
      "jupyter-console                   6.6.3\n",
      "jupyter_core                      5.5.0\n",
      "jupyter-events                    0.8.0\n",
      "jupyter-lsp                       2.2.0\n",
      "jupyter_server                    2.10.0\n",
      "jupyter_server_terminals          0.4.4\n",
      "jupyterlab                        4.0.11\n",
      "jupyterlab-pygments               0.1.2\n",
      "jupyterlab_server                 2.25.1\n",
      "jupyterlab-widgets                3.0.9\n",
      "keyring                           23.13.1\n",
      "kiwisolver                        1.4.4\n",
      "lazy_loader                       0.3\n",
      "lazy-object-proxy                 1.6.0\n",
      "lckr_jupyterlab_variableinspector 3.1.0\n",
      "libarchive-c                      2.9\n",
      "libmambapy                        1.5.6\n",
      "linkify-it-py                     2.0.0\n",
      "llvmlite                          0.42.0\n",
      "lmdb                              1.4.1\n",
      "locket                            1.0.0\n",
      "lxml                              4.9.3\n",
      "lz4                               4.3.2\n",
      "Markdown                          3.4.1\n",
      "markdown-it-py                    2.2.0\n",
      "MarkupSafe                        2.1.3\n",
      "matplotlib                        3.8.0\n",
      "matplotlib-inline                 0.1.6\n",
      "mccabe                            0.7.0\n",
      "mdit-py-plugins                   0.3.0\n",
      "mdurl                             0.1.0\n",
      "menuinst                          2.0.2\n",
      "mistune                           2.0.4\n",
      "mkl-fft                           1.3.8\n",
      "mkl-random                        1.2.4\n",
      "mkl-service                       2.4.0\n",
      "more-itertools                    10.1.0\n",
      "mpmath                            1.3.0\n",
      "msgpack                           1.0.3\n",
      "multidict                         6.0.4\n",
      "multipledispatch                  0.6.0\n",
      "munkres                           1.1.4\n",
      "mypy                              1.8.0\n",
      "mypy-extensions                   1.0.0\n",
      "navigator-updater                 0.4.0\n",
      "nbclient                          0.8.0\n",
      "nbconvert                         7.10.0\n",
      "nbformat                          5.9.2\n",
      "nest-asyncio                      1.6.0\n",
      "networkx                          3.1\n",
      "nltk                              3.8.1\n",
      "notebook                          7.0.8\n",
      "notebook_shim                     0.2.3\n",
      "numba                             0.59.0\n",
      "numexpr                           2.8.7\n",
      "numpy                             1.26.4\n",
      "numpydoc                          1.5.0\n",
      "openpyxl                          3.0.10\n",
      "overrides                         7.4.0\n",
      "packaging                         23.1\n",
      "pandas                            2.1.4\n",
      "pandocfilters                     1.5.0\n",
      "panel                             1.3.8\n",
      "param                             2.0.2\n",
      "paramiko                          2.8.1\n",
      "parsel                            1.8.1\n",
      "parso                             0.8.3\n",
      "partd                             1.4.1\n",
      "pathlib                           1.0.1\n",
      "pathspec                          0.10.3\n",
      "patsy                             0.5.3\n",
      "pexpect                           4.8.0\n",
      "pickleshare                       0.7.5\n",
      "pillow                            10.2.0\n",
      "pip                               23.3.1\n",
      "pkce                              1.0.3\n",
      "pkginfo                           1.9.6\n",
      "platformdirs                      3.10.0\n",
      "plotly                            5.9.0\n",
      "pluggy                            1.0.0\n",
      "ply                               3.11\n",
      "prometheus-client                 0.14.1\n",
      "prompt-toolkit                    3.0.43\n",
      "Protego                           0.1.16\n",
      "protobuf                          3.20.3\n",
      "psutil                            5.9.0\n",
      "ptyprocess                        0.7.0\n",
      "pure-eval                         0.2.2\n",
      "py-cpuinfo                        9.0.0\n",
      "pyarrow                           14.0.2\n",
      "pyasn1                            0.4.8\n",
      "pyasn1-modules                    0.2.8\n",
      "pycodestyle                       2.10.0\n",
      "pycosat                           0.6.6\n",
      "pycparser                         2.21\n",
      "pyct                              0.5.0\n",
      "pycurl                            7.45.2\n",
      "pydantic                          1.10.12\n",
      "pydeck                            0.8.0\n",
      "PyDispatcher                      2.0.5\n",
      "pydocstyle                        6.3.0\n",
      "pyerfa                            2.0.0\n",
      "pyflakes                          3.0.1\n",
      "Pygments                          2.15.1\n",
      "PyJWT                             2.4.0\n",
      "pylint                            2.16.2\n",
      "pylint-venv                       2.3.0\n",
      "pyls-spyder                       0.4.0\n",
      "PyMySQL                           1.1.0\n",
      "PyNaCl                            1.5.0\n",
      "pyodbc                            5.0.1\n",
      "pyOpenSSL                         24.0.0\n",
      "pyparsing                         3.0.9\n",
      "PyQt5                             5.15.10\n",
      "PyQt5-sip                         12.13.0\n",
      "PyQtWebEngine                     5.15.6\n",
      "PySimpleGUI                       4.60.5\n",
      "PySocks                           1.7.1\n",
      "pytest                            7.4.0\n",
      "python-dateutil                   2.8.2\n",
      "python-dotenv                     0.21.0\n",
      "python-json-logger                2.0.7\n",
      "python-lsp-black                  1.2.1\n",
      "python-lsp-jsonrpc                1.0.0\n",
      "python-lsp-server                 1.7.2\n",
      "python-slugify                    5.0.2\n",
      "python-snappy                     0.6.1\n",
      "pytoolconfig                      1.2.6\n",
      "pytz                              2023.3.post1\n",
      "pyviz_comms                       3.0.0\n",
      "pywavelets                        1.5.0\n",
      "pywin32                           305.1\n",
      "pywin32-ctypes                    0.2.0\n",
      "pywinpty                          2.0.10\n",
      "PyYAML                            6.0.1\n",
      "pyzmq                             25.1.2\n",
      "QDarkStyle                        3.0.2\n",
      "qstylizer                         0.2.2\n",
      "QtAwesome                         1.2.2\n",
      "qtconsole                         5.4.2\n",
      "QtPy                              2.4.1\n",
      "queuelib                          1.6.2\n",
      "referencing                       0.30.2\n",
      "regex                             2023.10.3\n",
      "requests                          2.31.0\n",
      "requests-file                     1.5.1\n",
      "requests-toolbelt                 1.0.0\n",
      "rfc3339-validator                 0.1.4\n",
      "rfc3986-validator                 0.1.1\n",
      "rich                              13.3.5\n",
      "rope                              1.7.0\n",
      "rpds-py                           0.10.6\n",
      "Rtree                             1.0.1\n",
      "ruamel.yaml                       0.17.21\n",
      "ruamel-yaml-conda                 0.17.21\n",
      "s3fs                              2023.10.0\n",
      "scikit-image                      0.22.0\n",
      "scikit-learn                      1.2.2\n",
      "scipy                             1.11.4\n",
      "Scrapy                            2.8.0\n",
      "seaborn                           0.12.2\n",
      "semver                            2.13.0\n",
      "Send2Trash                        1.8.2\n",
      "service-identity                  18.1.0\n",
      "setuptools                        68.2.2\n",
      "sip                               6.7.12\n",
      "six                               1.16.0\n",
      "smart-open                        5.2.1\n",
      "smmap                             4.0.0\n",
      "sniffio                           1.3.0\n",
      "snowballstemmer                   2.2.0\n",
      "sortedcontainers                  2.4.0\n",
      "soupsieve                         2.5\n",
      "Sphinx                            5.0.2\n",
      "sphinxcontrib-applehelp           1.0.2\n",
      "sphinxcontrib-devhelp             1.0.2\n",
      "sphinxcontrib-htmlhelp            2.0.0\n",
      "sphinxcontrib-jsmath              1.0.1\n",
      "sphinxcontrib-qthelp              1.0.3\n",
      "sphinxcontrib-serializinghtml     1.1.5\n",
      "spyder                            5.4.3\n",
      "spyder-kernels                    2.4.4\n",
      "SQLAlchemy                        2.0.25\n",
      "stack-data                        0.2.0\n",
      "statsmodels                       0.14.0\n",
      "streamlit                         1.30.0\n",
      "sympy                             1.12\n",
      "tables                            3.9.2\n",
      "tabulate                          0.9.0\n",
      "tblib                             1.7.0\n",
      "tenacity                          8.2.2\n",
      "terminado                         0.17.1\n",
      "text-unidecode                    1.3\n",
      "textdistance                      4.2.1\n",
      "threadpoolctl                     2.2.0\n",
      "three-merge                       0.1.1\n",
      "tifffile                          2023.4.12\n",
      "tinycss2                          1.2.1\n",
      "tldextract                        3.2.0\n",
      "toml                              0.10.2\n",
      "tomlkit                           0.11.1\n",
      "toolz                             0.12.0\n",
      "tornado                           6.3.3\n",
      "tqdm                              4.65.0\n",
      "traitlets                         5.7.1\n",
      "truststore                        0.8.0\n",
      "Twisted                           23.10.0\n",
      "twisted-iocpsupport               1.0.2\n",
      "typing_extensions                 4.9.0\n",
      "tzdata                            2023.3\n",
      "tzlocal                           2.1\n",
      "uc-micro-py                       1.0.1\n",
      "ujson                             5.4.0\n",
      "Unidecode                         1.2.0\n",
      "urllib3                           2.0.7\n",
      "validators                        0.18.2\n",
      "w3lib                             2.1.2\n",
      "watchdog                          2.1.6\n",
      "wcwidth                           0.2.5\n",
      "webencodings                      0.5.1\n",
      "websocket-client                  0.58.0\n",
      "Werkzeug                          2.2.3\n",
      "whatthepatch                      1.0.2\n",
      "wheel                             0.41.2\n",
      "widgetsnbextension                3.5.2\n",
      "win-inet-pton                     1.1.0\n",
      "wrapt                             1.14.1\n",
      "xarray                            2023.6.0\n",
      "xlwings                           0.29.1\n",
      "xyzservices                       2022.9.0\n",
      "yapf                              0.31.0\n",
      "yarl                              1.9.3\n",
      "zict                              3.0.0\n",
      "zipp                              3.17.0\n",
      "zope.interface                    5.4.0\n",
      "zstandard                         0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<method-wrapper '__repr__' of MySQLDatabase object at 0x000001B819D8BB90>\n",
      "john_doe john@example.com\n",
      "john_doe new_email@example.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1572\\2984792234.py:6: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1572\\2984792234.py:16: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import engine, create_engine, DateTime, Column, Integer, String, Enum\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Tausers(Base):\n",
    "    __tablename__ = \"tauser\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    username = Column(String)\n",
    "    email =  Column(String)\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n",
    "\n",
    "    Base = declarative_base()\n",
    "\n",
    "\n",
    "class MySQLDatabase:\n",
    "    def __init__(self, db_url):\n",
    "        self.engine = create_engine(db_url)\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "        # Base.metadata.create_all(self.engine)\n",
    "\n",
    "    def add_user(self, username, email):\n",
    "        session = self.Session()\n",
    "        user = Tausers(username=username, email=email)\n",
    "        session.add(user)\n",
    "        session.commit()\n",
    "        session.close()\n",
    "\n",
    "    def get_user_by_username(self, username):\n",
    "        session = self.Session()\n",
    "        user = session.query(Tausers).filter_by(username=username).first()\n",
    "        session.close()\n",
    "        return user\n",
    "\n",
    "    def get_user_by_email(self, email):\n",
    "        session = self.Session()\n",
    "        user = session.query(Tausers).filter_by(email=email).first()\n",
    "        session.close()\n",
    "        return user\n",
    "\n",
    "    def update_user_email(self, username, new_email):\n",
    "        session = self.Session()\n",
    "        user = session.query(Tausers).filter_by(username=username).first()\n",
    "        if user:\n",
    "            user.email = new_email\n",
    "            session.commit()\n",
    "        session.close()\n",
    "\n",
    "    def delete_user(self, username):\n",
    "        session = self.Session()\n",
    "        user = session.query(Tausers).filter_by(username=username).first()\n",
    "        if user:\n",
    "            session.delete(user)\n",
    "            session.commit()\n",
    "        session.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        user = \"root\"\n",
    "        password = \"\"\n",
    "        host = \"localhost\"\n",
    "        port = 3306\n",
    "        db = \"test\"\n",
    "\n",
    "        db_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\"\n",
    "        db = MySQLDatabase(db_url)\n",
    "\n",
    "        print(db.__repr__)\n",
    "\n",
    "        db.add_user(\"john_doe\", \"john@example.com\")\n",
    "        user =  db.get_user_by_username(\"john_doe\")\n",
    "        print(user.username, user.email)\n",
    "        db.update_user_email(\"john_doe\", \"new_email@example.com\")\n",
    "        user =  db.get_user_by_username(\"john_doe\")\n",
    "        print(user.username, user.email)\n",
    "\n",
    "        # db.delete_user(\"john_doe\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"DB부분처리안됨\")    \n",
    "        print(\"e\")\n",
    "\n",
    "\n",
    "# 디비명 test\n",
    "\n",
    "# CREATE TABLE `tauser` (\n",
    "#   `id` INT(10) NOT NULL AUTO_INCREMENT,\n",
    "#   `username` VARCHAR(50) NOT NULL DEFAULT '0' COLLATE 'utf8mb4_general_ci',\n",
    "#   `email` VARCHAR(50) NOT NULL DEFAULT '0' COLLATE 'utf8mb4_general_ci',\n",
    "#   `created_at` TIMESTAMP NULL DEFAULT NULL,\n",
    "#   `updated_at` TIMESTAMP NULL DEFAULT NULL,\n",
    "#   PRIMARY KEY (`id`) USING BTREE\n",
    "# )\n",
    "# COLLATE='utf8mb4_general_ci'\n",
    "# ENGINE=InnoDB\n",
    "# AUTO_INCREMENT=5\n",
    "# ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "title_list = []\n",
    "subtitle_list = []\n",
    "date_list = []\n",
    "\n",
    "#1. 포스트 리스트\n",
    "post_list = soup.find_all('div', {'class': 'post-preview'})\n",
    "\n",
    "#2. 제목, 소제목, 날짜 리스트\n",
    "for post in post_list:\n",
    "    title = post.find('h2', {'class' : 'post-title'}).text.strip()\n",
    "    subtitle = post.find('h3', {'class' : 'post-subtitle'}).text.strip()\n",
    "    date = post.find('p', {'class' : 'post-meta'}).text.strip()\n",
    "    title_list.append(title)\n",
    "    subtitle_list.append(subtitle)\n",
    "    date_list.append(date)\n",
    "\n",
    "# zip(title_list, subtitle_list, date_list)\n",
    "# df = pd.DataFrame(zip(title_list, subtitle_list, date_list), columns=[\"title\", \"subtitle\", \"date\"])\n",
    "\n",
    "#3. 데이터프레임 만들기\n",
    "df = pd.DataFrame({'title': title_list, 'subtitle': subtitle_list, 'date': date_list})\n",
    "df\n",
    "\n",
    "# df.to_csv(r\"C:\\Users\\USER\\Desktop\\workspace01\\skn_1\\crawling_\\크롤링예제\\결과.csv\", encoding='utf-8', header=True, sep=\",\" )\n",
    "# df.to_excel(r\"C:\\Users\\USER\\Desktop\\workspace01\\skn_1\\crawling_\\크롤링예제\\결과.xlsx\",\n",
    "#             #  excel_writer=\"pyopenxl\", \n",
    "#              index=False )\n",
    "\n",
    "# 파일로 저장 > csv, excel\n",
    "# db로 저장 (수집될때마다, 절차 스케줄) > mysql, oracle, sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<div class=\"alert alert-warning\">\n",
    "[Tip] CSS Selector 를 활용한 태그 선택\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#css 문법 \n",
    "# select = select_all\n",
    "# select_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"post-title\">\n",
       "               단풍구경가자~~\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               다이어트 시작!\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               꽃놀이간다!\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               내 꿀은 누가 다 먹었나?\n",
       "             </h2>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a > h2')   # soup.select('a h2') 로 작성해도 동일하게 동작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"post-title\">\n",
       "               단풍구경가자~~\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               다이어트 시작!\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               꽃놀이간다!\n",
       "             </h2>,\n",
       " <h2 class=\"post-title\">\n",
       "               내 꿀은 누가 다 먹었나?\n",
       "             </h2>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"post-title\">\n",
       "               단풍구경가자~~\n",
       "             </h2>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('body > div > div > div > div:nth-child(1) > a > h2')\n",
    "# soup.select(f'body > div > div > div > div:nth-child({1+1}) > a > h2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.4 실습-쇼핑몰 가격 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    [문제]  \n",
    "    \n",
    "    \n",
    "상품정보가 포함된 하나의 페이지를 파싱하여 상품정보가 포함된 데이터 프레임을 생성하세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "# https://www.neweracapkorea.com/shop/shopbrand.html?type=Y&xcode=031&mcode=002&sort=&page=1\n",
    "# http://www.neweracapkorea.com/shop/shopbrand.html?xcode=031&mcode=002&type=Y&gf_ref=Yz1vU0FlS3M=\n",
    "base_url = \"http://www.neweracapkorea.com\"\n",
    "cap_total_url = \"/shop/shopbrand.html?xcode=031&mcode=002&type=Y&gf_ref=Yz1vU0FlS3M=\"\n",
    "base_url + cap_total_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get html\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "response = requests.get(base_url+cap_total_url, headers=headers)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup을 활용하여 데이터 파싱\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# http://www.neweracapkorea.com/shop/shopbrand.html?xcode=031&mcode=002&type=Y&gf_ref=Yz1vU0FlS3M=\n",
    "base_url = \"http://www.neweracapkorea.com\"\n",
    "cap_total_url = \"/shop/shopbrand.html?xcode=031&mcode=002&type=Y&gf_ref=Yz1vU0FlS3M=\"\n",
    "base_url + cap_total_url\n",
    "\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "\n",
    "cap_info_list = []\n",
    "price_list = []\n",
    "url_list = []\n",
    "\n",
    "for ii in range(0, 9):\n",
    "    newurl = f\"https://www.neweracapkorea.com/shop/shopbrand.html?type=Y&xcode=031&mcode=002&sort=&page={ii+1}\"\n",
    "    # get html\n",
    "    response = requests.get(newurl, headers=headers)\n",
    "    response.status_code\n",
    "\n",
    "    # BeautifulSoup을 활용하여 데이터 파싱\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    soup\n",
    "\n",
    "    id = \"productClass\"\n",
    "\n",
    "\n",
    "    for i in range(0, 100):\n",
    "        for j in range(0, 5):\n",
    "            try:\n",
    "                cap_info_list.append(soup.select(f'#productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child({i+2}) > td:nth-child({j+1}) > div > ul > li.dsc')[0].text)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            try:        \n",
    "                price_list.append(soup.select(f\"#productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child({i+2}) > td:nth-child({j+1}) > div > ul > li.price\")[0].text)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            try:\n",
    "                url_list.append(soup.select(f\"#productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child({i+2}) > td:nth-child({j+1}) > div > div > a\")[0].attrs[\"href\"])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(zip(cap_info_list,price_list,url_list),columns=[\"상품명\",\"가격\",\"상세페이지\"])\n",
    "df.to_excel(\"뉴에라.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_url = \"http://www.neweracapkorea.com\"\n",
    "cap_total_url = \"/shop/shopbrand.html?xcode=031&mcode=002&type=Y&gf_ref=Yz1vU0FlS3M=\"\n",
    "base_url + cap_total_url\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "for ii in range(0,9):\n",
    "    newurl = f\"https://www.neweracapkorea.com/shop/shopbrand.html?type=Y&xcode=031&mcode=002&sort=&page={ii+1}\"\n",
    "\n",
    "    response = requests.get(newurl, headers=headers)\n",
    "    response.status_code\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    soup\n",
    "    # class . \n",
    "    # id #\n",
    "\n",
    "    id = \"productClass\"\n",
    "\n",
    "    # <div>class=\"page_body\">\n",
    "    # <div class=\"width1200\"></div>\n",
    "\n",
    "    #productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child(2) > td:nth-child(1) > div > ul > li.dsc\n",
    "\n",
    "    cap_info_list = []\n",
    "    price_list = []\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(0,20):\n",
    "        for j in range(0,5):    \n",
    "            try:        \n",
    "                cap_info_list.append(soup.select(f\"#productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child({i+2}) > td:nth-child({j+1}) > div > ul > li.dsc\")[0].text)  \n",
    "            except IndexError as e:\n",
    "                continue\n",
    "            try:  \n",
    "                price_list.append(soup.select(f\"#productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child({i+2}) > td:nth-child({j+1}) > div > ul > li.price\")[0].text)\n",
    "            except IndexError as e:\n",
    "                continue\n",
    "            try:    \n",
    "                url_list.append(soup.select(f\"#productClass > div > div.page-body > div.width1200 > div > table > tbody > tr:nth-child({i+2}) > td:nth-child({j+1}) > div > div > a\")[0].attrs[\"href\"])\n",
    "            except IndexError as e:\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(zip(cap_info_list,price_list, url_list), columns=[\"상품명\", \"가격\", \"상세페이지\"])\n",
    "\n",
    "df.to_excel(\"뉴에라.xlsx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_engine(\"sqlite:///test.db\")\n",
    "df.to_sql(name=\"test2\", con= con )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#url \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cap_url \u001b[38;5;241m=\u001b[39m cap_info[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cap_url)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#상품명 \u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "#url \n",
    "cap_url = cap_info[0].find('a').get('href')\n",
    "print(cap_url)\n",
    "\n",
    "#상품명 \n",
    "name = cap_info[0].find('li', {'class':'dsc'}).text.STrip()\n",
    "print(name)\n",
    "\n",
    "#가격 \n",
    "price = cap_info[0].find('li', {'class':'price'}).text\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m url_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cap \u001b[38;5;129;01min\u001b[39;00m cap_info:\n\u001b[1;32m----> 6\u001b[0m     name \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdsc\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      7\u001b[0m     price \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      8\u001b[0m     url \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "name_list = []\n",
    "price_list = []\n",
    "url_list = []\n",
    "\n",
    "for cap in cap_info:\n",
    "    name = cap.find('li', {'class':'dsc'}).text\n",
    "    price = cap.find('li', {'class':'price'}).text\n",
    "    url = cap.find('a').get('href')\n",
    "    print(\"이름: {}, 가격: {}\".format(name, price))\n",
    "    name_list.append(name)\n",
    "    price_list.append(price)\n",
    "    url_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"이름\": name_list, \"가격\" : price_list, \"url\": url_list})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    [문제]  \n",
    "    \n",
    "    \n",
    "앞의 데이터프레임에 저장한 상품 상세페이지의 URL을 참조하여, 각각의 상품정보의 상세정보를 \n",
    "포함하는 데이터 프레임을 생성하세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'url'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get html\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(base_url \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# BeautifulSoup을 활용하여 데이터 파싱\u001b[39;00m\n\u001b[0;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'url'"
     ]
    }
   ],
   "source": [
    "# get html\n",
    "response = requests.get(base_url + df['url'][0])\n",
    "\n",
    "# BeautifulSoup을 활용하여 데이터 파싱\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_name = soup.find('h3', {'class':'tit-prd'}).text.strip()\n",
    "cap_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_info = soup.find('tbody')\n",
    "features = cap_info.findAll('div', {'class':'tb-left'})\n",
    "\n",
    "for f in features:\n",
    "    print(f.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fic_list = []\n",
    "for f in features:\n",
    "    fic_list.append(f.text.strip())\n",
    "fic_list\n",
    "\n",
    "cap_dict = {'이름': cap_name}\n",
    "\n",
    "########## 추가부분 시작 ##################\n",
    "if len(fic_list) % 2 != 0:\n",
    "    fic_list = fic_list[1:] \n",
    "########## 추가부분 끝  ##################\n",
    "\n",
    "for i in range(len(fic_list)):\n",
    "    if i % 2 == 0:\n",
    "        cap_dict.update({fic_list[i]: fic_list[i+1]})\n",
    "\n",
    "cap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = pd.DataFrame(data = cap_dict,  index = [0])\n",
    "cap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_detail_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    print(i , end='... ')\n",
    "    # (1)df에 저장된 url 읽어오기\n",
    "    url = df['url'][i]\n",
    "    response = requests.get(base_url + url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "    # (2)상세페이지의 정보 파싱\n",
    "    cap_name = soup.find('h3', {'class':'tit-prd'}).text.strip()\n",
    "    cap_info = soup.find('tbody')\n",
    "    features = cap_info.findAll('div', {'class':'tb-left'})\n",
    "    \n",
    "    fic_list = []\n",
    "    for f in features:\n",
    "        fic_list.append(f.text.strip())\n",
    "    \n",
    "    # (3)파싱한 정보를 사용하여 딕셔너리 생성\n",
    "    cap_dict = {'제품명': cap_name}\n",
    "\n",
    "    ########## 추가부분 시작 ##################\n",
    "    if len(fic_list) % 2 != 0:\n",
    "        fic_list = fic_list[1:] \n",
    "    ########## 추가부분 끝  ##################\n",
    "\n",
    "    for i in range(len(fic_list)):\n",
    "        if i % 2 == 0:\n",
    "            if fic_list[i] != '':\n",
    "                cap_dict.update({fic_list[i]: fic_list[i+1]})\n",
    "    \n",
    "    # (4)생성한 딕셔너리로 데이터프레임 생성\n",
    "    temp_df = pd.DataFrame(data = cap_dict,  index = [0])\n",
    "\n",
    "    # (5) 데이터프레임 추가\n",
    "    if len(cap_detail_df) == 0:\n",
    "        cap_detail_df = temp_df\n",
    "    else:\n",
    "        cap_detail_df = cap_detail_df.append(temp_df, ignore_index = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_detail_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_detail_df['특이사항'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    [문제]  \n",
    "    \n",
    "    \n",
    "여러 페이지의 정보를 앞의 데이터프레임에 저장한 상품 상세페이지의 URL을 참조하여, 각각의 \n",
    "상품정보의 상세정보를 포함하는 데이터 프레임을 생성하세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://www.neweracapkorea.com\"\n",
    "\n",
    "name_list = []\n",
    "price_list = []\n",
    "url_list = []\n",
    "index = 1\n",
    "\n",
    "while True: \n",
    "    try :\n",
    "        print('{} 페이지 파싱.....'.format(index))\n",
    "        page_url = f\"/shop/shopbrand.html?type=Y&xcode=031&mcode=002&sort=&page={index}\"\n",
    "        response = requests.get(base_url+page_url)\n",
    "\n",
    "        # BeautifulSoup을 활용하여 데이터 파싱\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        cap_info = soup.findAll('div', {'class':'tb-center'})\n",
    "\n",
    "        if len(cap_info) == 0 :\n",
    "            print('끝')\n",
    "            break\n",
    "        for cap in cap_info:\n",
    "            name = cap.find('li', {'class':'dsc'}).text\n",
    "            price = cap.find('li', {'class':'price'})\n",
    "            url = cap.find('a').get('href')\n",
    "            #print(\"이름: {}, 가격: {}\".format(name, price))\n",
    "            name_list.append(name)\n",
    "            \n",
    "            if price != None :\n",
    "                price_list.append(price.text)\n",
    "            else :\n",
    "                price_list.append('SOLD OUT')\n",
    "            url_list.append(url)\n",
    "        index = index + 1\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"이름\": name_list, \"가격\" : price_list, \"url\": url_list})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://api.odcloud.kr/api/3075686/v1/uddi:76e7ab44-df42-4403-83a4-f0060345c77a?page=1&perPage=10&returnType=JSON&serviceKey=jGtWW7zfVyOETrcgdW%2BBN9u%2F0akDIk8KrrV6LgAW3vJ3OA88jFb%2FG0iPHCtl7v4OViw4qNMSiNR5sGRDpiKejg%3D%3D',\n",
       " 'https://api.odcloud.kr/api/3075686/v1/uddi:76e7ab44-df42-4403-83a4-f0060345c77a?page=2&perPage=10&returnType=JSON&serviceKey=jGtWW7zfVyOETrcgdW%2BBN9u%2F0akDIk8KrrV6LgAW3vJ3OA88jFb%2FG0iPHCtl7v4OViw4qNMSiNR5sGRDpiKejg%3D%3D']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://api.odcloud.kr/api/3075686/v1/uddi:76e7ab44-df42-4403-83a4-f0060345c77a?page=1&perPage=10&serviceKey=jGtWW7zfVyOETrcgdW%2BBN9u%2F0akDIk8KrrV6LgAW3vJ3OA88jFb%2FG0iPHCtl7v4OViw4qNMSiNR5sGRDpiKejg%3D%3D\n",
    "\n",
    "base_url = r\"https://api.odcloud.kr/api/3075686/v1/uddi:76e7ab44-df42-4403-83a4-f0060345c77a?\"\n",
    "query_string = r\"page=1&perPage=10&serviceKey=jGtWW7zfVyOETrcgdW%2BBN9u%2F0akDIk8KrrV6LgAW3vJ3OA88jFb%2FG0iPHCtl7v4OViw4qNMSiNR5sGRDpiKejg%3D%3D\"\n",
    "\n",
    "page = '1'\n",
    "\n",
    "perPage = '10'\n",
    "\n",
    "returnType = 'JSON'\n",
    "\n",
    "serviceKey = r\"serviceKey=jGtWW7zfVyOETrcgdW%2BBN9u%2F0akDIk8KrrV6LgAW3vJ3OA88jFb%2FG0iPHCtl7v4OViw4qNMSiNR5sGRDpiKejg%3D%3D\"\n",
    "\n",
    "targets = []\n",
    "\n",
    "for i in range(0, 2):\n",
    "    targets.append(base_url +'page='+ f\"{i+1}\" + \"&\" +'perPage='+ perPage + \"&\" + 'returnType='+ returnType + \"&\" + serviceKey)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "10\n",
      "10\n",
      "['청주시', '충주시', '제천시', '보은군', '옥천군', '영동군', '증평군', '진천군', '괴산군', '음성군']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "res = requests.get(targets[0])\n",
    "print(res.status_code)\n",
    "\n",
    "json_res1 = res.json()\n",
    "print(json_res1[\"currentCount\"])\n",
    "\n",
    "json_res2 = json.loads(res.text)\n",
    "print(json_res2[\"currentCount\"])\n",
    "\n",
    "# print(json_res1.get(\"data\")[1])\n",
    "구분 = []\n",
    "승용차 = []\n",
    "승합차 = []\n",
    "특수차 = []\n",
    "화물차 = []\n",
    "승용차 = []\n",
    "승합차 = []\n",
    "특수차 = []\n",
    "화물차 = []\n",
    "승용차 = []\n",
    "승합차 = []\n",
    "특수차 = []\n",
    "화물차 = []\n",
    "승용차 = []\n",
    "승합차 = []\n",
    "특수차 = []\n",
    "화물차 = []\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    구분.append(json_res1[\"data\"][i][\"구분\"])\n",
    "    승용차.append(json_res1[\"data\"][i][\"계(승용차)\"])\n",
    "    승합차.append(json_res1[\"data\"][i][\"계(승합차)\"])\n",
    "    특수차.append(json_res1[\"data\"][i][\"계(특수차)\"])\n",
    "    화물차.append(json_res1[\"data\"][i][\"계(화물차)\"])\n",
    "    승용차.append(json_res1[\"data\"][i][\"관용(승용차)\"])\n",
    "    승합차.append(json_res1[\"data\"][i][\"관용(승합차)\"])\n",
    "    특수차.append(json_res1[\"data\"][i][\"관용(특수차)\"])\n",
    "    화물차.append(json_res1[\"data\"][i][\"관용(화물차)\"])\n",
    "    승용차.append(json_res1[\"data\"][i][\"영업용(승용차)\"])\n",
    "    승합차.append(json_res1[\"data\"][i][\"영업용(승합차)\"])\n",
    "    특수차.append(json_res1[\"data\"][i][\"영업용(특수차)\"])\n",
    "    화물차.append(json_res1[\"data\"][i][\"영업용(화물차)\"])\n",
    "    승용차.append(json_res1[\"data\"][i][\"자가용(승용차)\"])\n",
    "    승합차.append(json_res1[\"data\"][i][\"자가용(승합차)\"])\n",
    "    특수차.append(json_res1[\"data\"][i][\"자가용(특수차)\"])\n",
    "    화물차.append(json_res1[\"data\"][i][\"자가용(화물차)\"])\n",
    "\n",
    "print(구분)\n",
    "df = pd.DataFrame(data = zip(\n",
    "    구분,\n",
    "    승용차,\n",
    "    승합차,\n",
    "    특수차,\n",
    "    화물차,\n",
    "    승용차,\n",
    "    승합차,\n",
    "    특수차,\n",
    "    화물차,\n",
    "    승용차,\n",
    "    승합차,\n",
    "    특수차,\n",
    "    화물차,\n",
    "    승용차,\n",
    "    승합차,\n",
    "    특수차,\n",
    "    화물차\n",
    "), \n",
    "columns = [\"구분\",   \n",
    "\"계(승용차)\",\n",
    "\"계(승합차)\",\n",
    "\"계(특수차)\",\n",
    "\"계(화물차)\",\n",
    "\"관용(승용차)\",\n",
    "\"관용(승합차)\",\n",
    "\"관용(특수차)\",\n",
    "\"관용(화물차)\",\n",
    "\"영업용(승용차)\",\n",
    "\"영업용(승합차)\",\n",
    "\"영업용(특수차)\",\n",
    "\"영업용(화물차)\",\n",
    "\"자가용(승용차)\",\n",
    "\"자가용(승합차)\",\n",
    "\"자가용(특수차)\",\n",
    "\"자가용(화물차)\"])\n",
    "\n",
    "df.to_excel(r\"C:\\Users\\USER\\Desktop\\workspace01\\skn_1\\crawling_\\크롤링예제\\data\\충북_2023.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.21.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.21.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.5 MB 5.1 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.4/9.5 MB 5.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/9.5 MB 7.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/9.5 MB 8.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.8/9.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.3/9.5 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.6/9.5 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 19.5 MB/s eta 0:00:00\n",
      "Downloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 467.7/467.7 kB 28.6 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Installing collected packages: h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-23.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.21.0 trio-0.25.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
